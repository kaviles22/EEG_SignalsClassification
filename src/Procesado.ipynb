{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declaracion de constantes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_MIN = 5\n",
    "F_MAX = 40\n",
    "F_MUESTREO = 128\n",
    "RUTA_DATOS_PROCESADOS = '../DatosProcesados/Frecuencia'\n",
    "CARPETAS = ['Rojo', 'Azul', 'Verde', 'Morado', 'Baseline']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Declaracion de Funciones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_datos(datos):\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(datos[:, 0], label = 'EEG.O1')\n",
    "    plt.plot(datos[:, 1], label = 'EEG.O2')\n",
    "    plt.legend()\n",
    "    plt.title('Raw Data')\n",
    "    plt.xlabel('Tiempo')\n",
    "    plt.ylabel('uV')\n",
    "    \n",
    "def plot_psd_semilog(datos):\n",
    "    \n",
    "    f1, psd_O1 = signal.periodogram(datos[:, 0], F_MUESTREO)\n",
    "    f2, psd_O2 = signal.periodogram(datos[:, 1], F_MUESTREO)\n",
    "    plt.semilogy(f1, psd_O1, label = 'EEG.O1')\n",
    "    plt.semilogy(f1, psd_O2, label = 'EEG.O2')\n",
    "    plt.ylim([1e-7, 1e2])\n",
    "    plt.xlabel('Frecuencia [Hz]')\n",
    "    plt.ylabel('PSD [V**2/Hz]')\n",
    "    plt.legend()\n",
    "\n",
    "    \n",
    "def plot_psd(datos):\n",
    "    \n",
    "    plt.figure()\n",
    "    f1, psd_O1 = signal.periodogram(datos[:, 0], F_MUESTREO)\n",
    "    f2, psd_O2 = signal.periodogram(datos[:, 1], F_MUESTREO)\n",
    "    plt.plot(f1, psd_O1, label = 'EEG.O1')\n",
    "    plt.plot(f1, psd_O2, label = 'EEG.O2')\n",
    "    plt.xlabel('Frecuencia [Hz]')\n",
    "    plt.ylabel('PSD')\n",
    "    plt.legend()\n",
    "    print(psd_O1.mean())\n",
    "    print(psd_O2.mean())\n",
    "    print(max(psd_O1))\n",
    "    print(max(psd_O2))\n",
    "\n",
    "def filtro_pasa_banda(datos, f_min, f_max, f_muestreo, orden=5):\n",
    "    nyq = 0.5 * f_muestreo\n",
    "    f_min = f_min / nyq\n",
    "    f_max = f_max / nyq\n",
    "    b, a = signal.butter(orden, [f_min, f_max], btype='band')\n",
    "    print(b, a)\n",
    "    y = signal.lfilter(b, a, datos)\n",
    "    return y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creacion de Archivos Transformada de Hilbert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import fftpack \n",
    "\n",
    "for carpeta in CARPETAS:\n",
    "    archivos = [e for e in os.listdir(RUTA_DATOS_PROCESADOS+'/'+carpeta) if e.endswith('.csv') ]\n",
    "    \n",
    "    for indice, archivo in enumerate(archivos):\n",
    "        ruta_origen = f'{RUTA_DATOS_PROCESADOS}/{carpeta}/{archivo}'\n",
    "        df = pd.read_csv(ruta_origen)\n",
    "        etiqueta = df[['MarkerValueInt']]\n",
    "        datos = df[['EEG.O1', 'EEG.O2']].to_numpy()\n",
    "        o1 = fftpack.hilbert(datos[:, 0])\n",
    "        o1 = pd.DataFrame(data=o1, columns=[\"EEG.O1\"])\n",
    "        o2 = fftpack.hilbert(datos[:, 1])\n",
    "        o2 = pd.DataFrame(data=o2, columns=[\"EEG.O2\"])\n",
    "        archivo = pd.concat([o1, o2, etiqueta], axis=1).to_csv(f'{RUTA_DATOS_PROCESADOS}/{carpeta}/Hilbert/hilbert{indice}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm ../DatosProcesados/Frecuencia/Baseline/hilbert*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features1 = {'Decil 1': [], 'Decil 2': [], 'Decil 3':[], 'Decil 4': [], 'Decil 5': [], 'Decil 6': [], 'Decil 7': [],'Decil 8': [], 'Decil 9': []}\n",
    "features2 = {'Decil 1': [], 'Decil 2': [], 'Decil 3':[], 'Decil 4': [], 'Decil 5': [], 'Decil 6': [], 'Decil 7': [],'Decil 8': [], 'Decil 9': []}\n",
    "for carpeta in CARPETAS:\n",
    "    archivos = [e for e in os.listdir(RUTA_DATOS_PROCESADOS+'/'+carpeta+'/FiltroPasoBanda') if e.endswith('.csv') ]\n",
    "    \n",
    "    for indice, archivo in enumerate(archivos):\n",
    "        ruta_origen = f'{RUTA_DATOS_PROCESADOS}/{carpeta}/FiltroPasoBanda/{archivo}'\n",
    "        df = pd.read_csv(ruta_origen)\n",
    "    \n",
    "        features1['Decil 1'].append(df.quantile(.1, axis = 0)[0])\n",
    "        features1['Decil 2'].append(df.quantile(.2, axis = 0)[0])\n",
    "        features1['Decil 3'].append(df.quantile(.3, axis = 0)[0])\n",
    "        features1['Decil 4'].append(df.quantile(.4, axis = 0)[0])\n",
    "        features1['Decil 5'].append(df.quantile(.5, axis = 0)[0])\n",
    "        features1['Decil 6'].append(df.quantile(.6, axis = 0)[0])\n",
    "        features1['Decil 7'].append(df.quantile(.7, axis = 0)[0])\n",
    "        features1['Decil 8'].append(df.quantile(.8, axis = 0)[0])\n",
    "        features1['Decil 9'].append(df.quantile(.9, axis = 0)[0])\n",
    "        \n",
    "    \n",
    "        features2['Decil 1'].append(df.quantile(.1, axis = 0)[0])\n",
    "        features2['Decil 2'].append(df.quantile(.2, axis = 0)[0])\n",
    "        features2['Decil 3'].append(df.quantile(.3, axis = 0)[0])\n",
    "        features2['Decil 4'].append(df.quantile(.4, axis = 0)[0])\n",
    "        features2['Decil 5'].append(df.quantile(.5, axis = 0)[0])\n",
    "        features2['Decil 6'].append(df.quantile(.6, axis = 0)[0])\n",
    "        features2['Decil 7'].append(df.quantile(.7, axis = 0)[0])\n",
    "        features2['Decil 8'].append(df.quantile(.8, axis = 0)[0])\n",
    "        features2['Decil 9'].append(df.quantile(.9, axis = 0)[0])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "O1 = pd.DataFrame(features1)\n",
    "O2 = pd.DataFrame(features2)\n",
    "ruta_origen = f'../Features/FiltroPasoBanda/'\n",
    "df1 = pd.read_csv(ruta_origen+'Features_EEG.O1.csv')\n",
    "df2 = pd.read_csv(ruta_origen+'Features_EEG.O2.csv')\n",
    "pd.concat([df1, O1], axis = 1).to_csv(f'{ruta_origen}Occipital1.csv', index = False)\n",
    "pd.concat([df2, O2], axis = 1).to_csv(f'{ruta_origen}Occipital2.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features1 = {'AH0': [], 'AH1': [],'AH2': [],'AH3': [],'AH4': [],'AH5': [],'AH6': [],'AH7': [],'AH8': [], 'AH9': []}\n",
    "features2 = {'AH0': [], 'AH1': [],'AH2': [],'AH3': [],'AH4': [],'AH5': [],'AH6': [],'AH7': [],'AH8': [], 'AH9': []}\n",
    "for carpeta in CARPETAS:\n",
    "    archivos = [e for e in os.listdir(RUTA_DATOS_PROCESADOS+'/'+carpeta+'/FiltroPasoBanda') if e.endswith('.csv') ]\n",
    "    \n",
    "    for indice, archivo in enumerate(archivos):\n",
    "        ruta_origen = f'{RUTA_DATOS_PROCESADOS}/{carpeta}/FiltroPasoBanda/{archivo}'\n",
    "        df = pd.read_csv(ruta_origen)\n",
    "        a1, b1 = np.histogram(df['EEG.O1'], density=True)\n",
    "        a2, b2 = np.histogram(df['EEG.O2'], density=True)\n",
    "        \n",
    "        features1['AH0'].append(a1[0])\n",
    "        features1['AH1'].append(a1[1])\n",
    "        features1['AH2'].append(a1[2])\n",
    "        features1['AH3'].append(a1[3])\n",
    "        features1['AH4'].append(a1[4])\n",
    "        features1['AH5'].append(a1[5])\n",
    "        features1['AH6'].append(a1[6])\n",
    "        features1['AH7'].append(a1[7])\n",
    "        features1['AH8'].append(a1[8])\n",
    "        features1['AH9'].append(a1[9])\n",
    "        \n",
    "        features2['AH0'].append(a2[0])\n",
    "        features2['AH1'].append(a2[1])\n",
    "        features2['AH2'].append(a2[2])\n",
    "        features2['AH3'].append(a2[3])\n",
    "        features2['AH4'].append(a2[4])\n",
    "        features2['AH5'].append(a2[5])\n",
    "        features2['AH6'].append(a2[6])\n",
    "        features2['AH7'].append(a2[7])\n",
    "        features2['AH8'].append(a2[8])\n",
    "        features2['AH9'].append(a2[9]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_origen = f'../Features/FiltroPasoBanda/'\n",
    "O1 = pd.DataFrame(features1)\n",
    "O2 = pd.DataFrame(features2)\n",
    "df1 = pd.read_csv(ruta_origen+'Occipital1.csv')\n",
    "df2 = pd.read_csv(ruta_origen+'Occipital2.csv')\n",
    "pd.concat([df1, O1], axis = 1).to_csv(f'{ruta_origen}Occipital1.csv', index = False)\n",
    "pd.concat([df2, O2], axis = 1).to_csv(f'{ruta_origen}Occipital2.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split DF en train y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(ruta_origen+'Occipital1.csv')\n",
    "df2 = pd.read_csv(ruta_origen+'Occipital2.csv')\n",
    "\n",
    "df1 = df1.sample(frac=1).reset_index(drop=True)\n",
    "df2 = df2.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "df1.loc[0:1253, :].to_csv(f'{ruta_origen}train/Occipital1.csv', index = False)\n",
    "df1.loc[1254:, :].to_csv(f'{ruta_origen}test/Occipital1.csv', index = False)\n",
    "df2.loc[0:1253, :].to_csv(f'{ruta_origen}train/Occipital2.csv', index = False)\n",
    "df2.loc[1254:, :].to_csv(f'{ruta_origen}test/Occipital2.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "df1 = pd.read_csv(ruta_origen+'train/Occipital1.csv')\n",
    "df2 = pd.read_csv(ruta_origen+'train/Occipital2.csv')\n",
    "x1 = df1.values #returns a numpy array\n",
    "x2 = df2.values\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x1_scaled = min_max_scaler.fit_transform(x1)\n",
    "x2_scaled = min_max_scaler.fit_transform(x2)\n",
    "train1_norm = pd.DataFrame(x1_scaled, columns=list(df1))\n",
    "train2_norm = pd.DataFrame(x2_scaled, columns=list(df2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guardar valores maximos y minimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max1 = df1.max(axis=0)\n",
    "max1 = pd.DataFrame(max1, columns=[\"Max\"])\n",
    "\n",
    "min1 = df1.min(axis=0)\n",
    "min1 = pd.DataFrame(min1, columns=[\"Min\"])\n",
    "\n",
    "max2 = df2.max(axis=0)\n",
    "max2 = pd.DataFrame(max2, columns=[\"Max\"])\n",
    "\n",
    "min2 = df2.min(axis=0)\n",
    "min2 = pd.DataFrame(min2, columns=[\"Min\"])\n",
    "\n",
    "pd.concat([max1, min1], axis = 1).to_csv(f'{ruta_origen}Valores_O1.csv', index = False)\n",
    "pd.concat([max2, min2], axis = 1).to_csv(f'{ruta_origen}Valores_O2.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Varianza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features before variance thresholding: 41\n",
      "Number of features after variance thresholding: 1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "selector = VarianceThreshold(threshold=0.1)\n",
    "train1_norm_new = selector.fit_transform(train1_norm)\n",
    "\n",
    "print('Number of features before variance thresholding: {}'.format(train1_norm.shape[1]))\n",
    "print('Number of features after variance thresholding: {}'.format(train1_norm_new.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "{'AH2', 'Varianza', 'Decil 3', 'Decil 8', 'AH3', 'Decil 4', 'valorMediaDiferenciaAbs', 'AH9', 'FHist4', 'valorAbsMediana', 'Decil 9', 'valorAbsPromWII', 'FHist7', 'Decil 6', 'VarianzaFrecCentral', 'valorAbsPromWI', 'FHist9', 'Decil 7', 'AH7', 'AH8', 'FHist10', 'FHist8', 'AH1', 'Decil 2', 'PSD_max'}\n"
     ]
    }
   ],
   "source": [
    "correlated_features = set()\n",
    "correlation_matrix = train1_norm.corr()\n",
    "for i in range(len(correlation_matrix .columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.8:\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            correlated_features.add(colname)\n",
    "            \n",
    "print(len(correlated_features))\n",
    "print(correlated_features)\n",
    "train1_norm.drop(labels=correlated_features, axis=1, inplace=True)\n",
    "train1_norm.to_csv(f'{ruta_origen}train/Occipital1.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "{'AH2', 'Decil 3', 'Decil 8', 'AH3', 'Decil 4', 'valorMediaDiferenciaAbs', 'FHist4', 'valorAbsMediana', 'Decil 9', 'valorAbsPromWII', 'FHist7', 'Decil 6', 'VarianzaFrecCentral', 'valorAbsPromWI', 'FHist9', 'Decil 7', 'AH7', 'AH8', 'FHist10', 'FHist8', 'AH1', 'Decil 2', 'PSD_max'}\n"
     ]
    }
   ],
   "source": [
    "correlated_features = set()\n",
    "correlation_matrix = train2_norm.corr()\n",
    "for i in range(len(correlation_matrix .columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.8:\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            correlated_features.add(colname)\n",
    "            \n",
    "print(len(correlated_features))\n",
    "print(correlated_features)\n",
    "train2_norm.drop(labels=correlated_features, axis=1, inplace=True)\n",
    "train2_norm.to_csv(f'{ruta_origen}train/Occipital2.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
